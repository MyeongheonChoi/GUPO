import torch
import hydra
import os
import tqdm
import wandb
from collections import defaultdict
from omegaconf import DictConfig, OmegaConf

# --- 1. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ í•„ìš”í•œ ëª¨ë“ˆë“¤ ì„í¬íŠ¸ ---
# (ê²½ë¡œëŠ” ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)
from trainers.gupo_trainers import GUPOTrainer
from preference_datasets import get_batch_iterator
from utils import (                        
    rank0_print,
    get_local_dir,
    slice_and_move_batch_for_device,
    formatted_dict
)
import transformers
from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training

# (train.pyì—ì„œ ëª¨ë¸ ë¡œë“œì— ì‚¬ìš©í•˜ë˜ ë‹¤ë¥¸ í•¨ìˆ˜ë“¤ë„ í•„ìš”ì‹œ ì„í¬íŠ¸)


def load_model_and_tokenizer(config: DictConfig):
    """
    í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    rank0_print(f"Loading tokenizer from {config.model.tokenizer_name_or_path}")
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        config.model.tokenizer_name_or_path or config.model.name_or_path
    )
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    rank0_print(f"Loading policy model: {config.model.name_or_path}")
    policy = transformers.AutoModelForCausalLM.from_pretrained(
        config.model.name_or_path,
        torch_dtype=torch.bfloat16,  # bfloat16 ì‚¬ìš©
        # (bitsandbytes 4/8bit ë¡œë“œ ì„¤ì •ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€)
    )

    if config.lora.enabled:
        rank0_print("Applying LoRA adapters to policy model...")
        
        # PEFT JSON ì§ë ¬í™” ì˜¤ë¥˜ ë°©ì§€ (ListConfig -> list)
        target_modules_list = list(config.lora.target_modules)
        
        lora_config = LoraConfig(
            r=config.lora.r,
            target_modules=target_modules_list,
            lora_alpha=config.lora.alpha,
            lora_dropout=config.lora.dropout,
            bias="none",
            task_type="CAUSAL_LM"
        )
        policy = get_peft_model(policy, lora_config)
        rank0_print("LoRA applied. Trainable parameters:")
        policy.print_trainable_parameters()

    rank0_print(f"Loading reference model: {config.model.name_or_path}")
    reference_model = transformers.AutoModelForCausalLM.from_pretrained(
        config.model.name_or_path,
        torch_dtype=torch.bfloat16, # bfloat16 ì‚¬ìš©
    )
    
    return policy, reference_model, tokenizer


@hydra.main(config_path="configs", config_name="config") # train.pyì™€ ë™ì¼í•œ config ì‚¬ìš©
def main(config: DictConfig):
    """
    ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ì—¬ MLP Beta ë° ê¸°íƒ€ ì§€í‘œë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    """
    
    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    policy, reference_model, tokenizer = load_model_and_tokenizer(config)
    
    # 2. BasicTrainers ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    #    (BasicTrainersì˜ __init__ì´ mlp, eval_batches ë“±ì„ ìƒì„±)
    rank0_print("Initializing BasicTrainers...")
    trainer = GUPOTrainer(
        policy=policy,
        config=config,
        seed=config.seed,
        run_dir=f"evaluation_run_{config.checkpoint_dir.split('/')[-1]}", # ì„ì‹œ ì‹¤í–‰ ë””ë ‰í† ë¦¬
        reference_model=reference_model,
        tokenizer=tokenizer, # __init__ì—ì„œ í•„ìš”í•˜ë©´ ì „ë‹¬
        rank=0,         # ë‹¨ì¼ GPU í‰ê°€ ê°€ì •
        world_size=1
    )

    # 3. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint_dir = config.checkpoint_dir
    if not checkpoint_dir:
        raise ValueError("configì— 'checkpoint_dir'ì„(ë¥¼) ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

    rank0_print(f"Loading checkpoints from {checkpoint_dir}...")
    
    # Policy (LoRA ì–´ëŒ‘í„°) ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    policy_checkpoint_path = os.path.join(checkpoint_dir, 'policy.pt')
    if os.path.exists(policy_checkpoint_path):
        policy_checkpoint = torch.load(policy_checkpoint_path, map_location='cpu')
        trainer.policy.load_state_dict(policy_checkpoint['state'])
        rank0_print(f"Loaded policy checkpoint from step {policy_checkpoint.get('step_idx', 'N/A')}")
    else:
        rank0_print(f"Warning: policy.pt not found in {checkpoint_dir}. Using base LoRA.")

    # MLP ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    mlp_checkpoint_path = os.path.join(checkpoint_dir, 'mlp.pt')
    if os.path.exists(mlp_checkpoint_path):
        mlp_checkpoint = torch.load(mlp_checkpoint_path, map_location='cpu')
        trainer.mlp.load_state_dict(mlp_checkpoint['state'])
        rank0_print(f"Loaded MLP checkpoint from step {mlp_checkpoint.get('step_idx', 'N/A')}")
    else:
        raise FileNotFoundError(f"mlp.pt not found in {checkpoint_dir}. MLP í‰ê°€ì— í•„ìˆ˜ì…ë‹ˆë‹¤.")

    # 4. ëª¨ë¸ì„ GPUë¡œ ì´ë™
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    trainer.policy.to(device)
    trainer.reference_model.to(device)
    trainer.mlp.to(device)
    
    # 5. MLP ë² íƒ€ í‰ê°€ ì‹¤í–‰ (vLLM ì—†ì´)
    #    (train í•¨ìˆ˜ì—ì„œ ë³µì‚¬í•´ ì˜¨ evaluate_mlp í•¨ìˆ˜ ì‚¬ìš©)
    mean_eval_metrics = trainer.evaluate_mlp()
    
    rank0_print("--- ğŸ“Š Final Evaluation Metrics ---")
    rank0_print(formatted_dict(mean_eval_metrics))
    rank0_print("--- ---------------------------- ---")
    

    rank0_print("âœ… MLP Beta evaluation complete.")

if __name__ == "__main__":
    main()