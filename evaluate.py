import torch
import hydra
import os
import tqdm
import wandb
from collections import defaultdict
from omegaconf import DictConfig, OmegaConf
from transformers import BitsAndBytesConfig

# --- 1. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ í•„ìš”í•œ ëª¨ë“ˆë“¤ ì„í¬íŠ¸ ---
# (ê²½ë¡œëŠ” ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)
from trainers.gupo_trainers import GUPOTrainer
from preference_datasets import get_batch_iterator
from utils import (                        
    rank0_print,
    get_local_dir,
    slice_and_move_batch_for_device,
    formatted_dict,
    disable_dropout
)
import transformers
from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training

# (train.pyì—ì„œ ëª¨ë¸ ë¡œë“œì— ì‚¬ìš©í•˜ë˜ ë‹¤ë¥¸ í•¨ìˆ˜ë“¤ë„ í•„ìš”ì‹œ ì„í¬íŠ¸)


@hydra.main(config_path="outputs/gupo_joint", config_name="config") # train.pyì™€ ë™ì¼í•œ config ì‚¬ìš©
def main(config: DictConfig):
    """
    ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ì—¬ MLP Beta ë° ê¸°íƒ€ ì§€í‘œë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    """
    
    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print('building policy')
    model_kwargs = {'device_map': 'balanced'}
    if config.model.policy_quantization == '8bit':
        print('using 8-bit quantization for policy model')
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True
        )
        model_kwargs['quantization_config'] = bnb_config
    policy_dtype = getattr(torch, config.model.policy_dtype)
    policy = transformers.AutoModelForCausalLM.from_pretrained(
        config.model.name_or_path, 
        cache_dir=get_local_dir(config.local_dirs), 
        low_cpu_mem_usage=True, 
        dtype=policy_dtype, 
        **model_kwargs
    )
    
    if config.lora.enabled:
        print('applying LoRA adapters')
        if getattr(policy, 'is_loaded_in_8bit', False) or getattr(policy, 'is_loaded_in_4bit', False):
            policy = prepare_model_for_kbit_training(policy)
            
        lora_config = LoraConfig(
            r=config.lora.r,
            target_modules=list(config.lora.target_modules),
            lora_alpha=config.lora.alpha,
            lora_dropout=config.lora.dropout,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        policy = get_peft_model(policy, lora_config)
        policy.print_trainable_parameters()
    
    disable_dropout(policy)

    print('building reference model')
    reference_model_dtype = getattr(torch, config.model.reference_dtype)
    reference_model = transformers.AutoModelForCausalLM.from_pretrained(
        config.model.name_or_path, 
        cache_dir=get_local_dir(config.local_dirs), 
        low_cpu_mem_usage=True, 
        dtype=reference_model_dtype, 
        **model_kwargs
    )
    disable_dropout(reference_model)

    # 2. BasicTrainers ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    #    (BasicTrainersì˜ __init__ì´ mlp, eval_batches ë“±ì„ ìƒì„±)
    checkpoint_dir = "/home/mhchoi/GUPO/outputs/gupo_joint"
    if not checkpoint_dir:
        raise ValueError("configì— 'checkpoint_dir'ì„(ë¥¼) ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

    rank0_print("Initializing BasicTrainers...")
    trainer = GUPOTrainer(
        policy=policy,
        config=config,
        seed=config.seed,
        run_dir=f"gupo_mlp_evaluation_{checkpoint_dir.split('/')[-1]}", # ì„ì‹œ ì‹¤í–‰ ë””ë ‰í† ë¦¬
        reference_model=reference_model,
    )

    # 3. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    

    rank0_print(f"Loading checkpoints from {checkpoint_dir}...")
    
    # Policy (LoRA ì–´ëŒ‘í„°) ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    policy_checkpoint_path = os.path.join(checkpoint_dir, 'policy.pt')
    if os.path.exists(policy_checkpoint_path):
        policy_checkpoint = torch.load(policy_checkpoint_path, map_location='cpu')
        trainer.policy.load_state_dict(policy_checkpoint['state'])
        rank0_print(f"Loaded policy checkpoint from step {policy_checkpoint.get('step_idx', 'N/A')}")
    else:
        raise FileNotFoundError(f"Warning: policy.pt not found in {checkpoint_dir}.")

    # MLP ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    mlp_checkpoint_path = os.path.join(checkpoint_dir, 'mlp.pt')
    if os.path.exists(mlp_checkpoint_path):
        mlp_checkpoint = torch.load(mlp_checkpoint_path, map_location='cpu')
        trainer.mlp.load_state_dict(mlp_checkpoint['state'])
        rank0_print(f"Loaded MLP checkpoint from step {mlp_checkpoint.get('step_idx', 'N/A')}")
    else:
        raise FileNotFoundError(f"mlp.pt not found in {mlp_checkpoint_path}. MLP í‰ê°€ì— í•„ìˆ˜ì…ë‹ˆë‹¤.")

    # 4. ëª¨ë¸ì„ GPUë¡œ ì´ë™
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # trainer.policy.to(device)
    # trainer.reference_model.to(device)
    trainer.mlp.to(device)
    
    # 5. MLP ë² íƒ€ í‰ê°€ ì‹¤í–‰ (vLLM ì—†ì´)
    #    (train í•¨ìˆ˜ì—ì„œ ë³µì‚¬í•´ ì˜¨ evaluate_mlp í•¨ìˆ˜ ì‚¬ìš©)
    mean_eval_metrics = trainer.evaluate_mlp()
    
    rank0_print("--- ğŸ“Š Final Evaluation Metrics ---")
    rank0_print(formatted_dict(mean_eval_metrics))
    rank0_print("--- ---------------------------- ---")
    

    rank0_print("âœ… MLP Beta evaluation complete.")

if __name__ == "__main__":
    main()