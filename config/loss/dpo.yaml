# do DPO preference-based training
name: dpo

# the temperature parameter for DPO; lower values mean we care less about
#   the reference model
beta: 0.9

# if true, use a uniform (maximum entropy) reference model
reference_free: false

# divergence type; currently only "kl, reverse kl and jsd" are supported
divergence: reverse_kl

# if use alpha divergence, we need to specify alpha
alpha: 0

eta: 1.0
