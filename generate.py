import argparse
import os
import json
import torch
from omegaconf import OmegaConf
from datasets import load_dataset
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest


def main(args):
    # 1. í•™ìŠµ Config ë¡œë“œ
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    checkpoint_dir = os.path.abspath(args.checkpoint_dir)
    if not os.path.exists(checkpoint_dir):
        raise FileNotFoundError(f"Checkpoint directory not found: {checkpoint_dir}")
    
    parent_dir = os.path.dirname(checkpoint_dir)
    config_dir = os.path.join(parent_dir, "config.yaml")
    config = OmegaConf.load(config_dir)
    

    base_model_path = config.model.name_or_path
    print(f"ğŸš€ Initializing vLLM Engine with base model: {base_model_path}")
    
    # LoRA ì„¤ì • í™•ì¸
    enable_lora = config.lora.enabled
    
    llm = LLM(
        model=base_model_path,
        enable_lora=enable_lora,
        dtype="bfloat16",
        seed=config.seed,
        gpu_memory_utilization=0.9,
        max_model_len=args.max_len if args.max_len else config.model.get('max_length', 2048), # Config ì—†ìœ¼ë©´ args ì‚¬ìš©
    )

    # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° (ìƒì„± ì‹œì—ë§Œ ì“°ì´ëŠ” ì„¤ì •ì´ë¯€ë¡œ argsë¡œ ë°›ìŒ)
    sampling_params = SamplingParams(
        n=args.n_samples,
        temperature=args.temperature,
        top_p=args.top_p,
        skip_special_tokens=True,
        max_tokens=args.max_new_tokens,
        stop_token_ids=[llm.get_tokenizer().eos_token_id]
    )

    # LoRA ìš”ì²­ ê°ì²´ ìƒì„±
    lora_request = None
    if enable_lora:
        # ì–´ëŒ‘í„° ê²½ë¡œëŠ” ì²´í¬í¬ì¸íŠ¸ í´ë” ì•ˆì˜ 'adapter' í´ë”ë¡œ ìë™ ì§€ì •
        adapter_path = os.path.join(checkpoint_dir, 'adapter')
        
        if not os.path.exists(adapter_path):
             raise FileNotFoundError(f"âŒ LoRA is enabled in config, but adapter not found at: {adapter_path}")

        print(f"âœ… LoRA Adapter will be applied from: {adapter_path}")
        lora_request = LoRARequest("gupo_adapter", 1, adapter_path)

    # ------------------------------------------------------------------
    # 3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ (ìƒì„± ëŒ€ìƒ)
    # ------------------------------------------------------------------
    dataset_name = args.dataset_name or config.datasets[0] # argsê°€ ì—†ìœ¼ë©´ í•™ìŠµ ë°ì´í„°ì…‹ ì‚¬ìš© (ì„ íƒ)
    print(f"ğŸ“‚ Loading dataset: {dataset_name} (split: {args.split})")
    
    if dataset_name.endswith(".json") or dataset_name.endswith(".jsonl"):
        dataset = load_dataset("json", data_files=dataset_name, split=args.split)
    else:
        dataset = load_dataset(dataset_name, split=args.split)

    # í”„ë¡¬í”„íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
    prompt_col = args.prompt_column
    if prompt_col not in dataset.column_names:
        if 'prompt' in dataset.column_names: prompt_col = 'prompt'
        elif 'instruction' in dataset.column_names: prompt_col = 'instruction'
        else: raise ValueError(f"Dataset columns {dataset.column_names} do not contain '{prompt_col}' key.")
            
    prompts = dataset[prompt_col]
    print(f"ğŸ“Š Total samples to generate: {len(prompts)}")

    # ------------------------------------------------------------------
    # 4. ë¬¸ì¥ ìƒì„± & ì €ì¥
    # ------------------------------------------------------------------
    print("âš¡ Starting generation...")
    outputs = llm.generate(
        prompts, 
        sampling_params, 
        lora_request=lora_request
    )

    results = []
    for output in outputs:
        results.append({
            "prompt": output.prompt,
            "generated_response": output.outputs[0].text
        })

    # ì €ì¥ ê²½ë¡œ: ì²´í¬í¬ì¸íŠ¸ í´ë” ì•ˆì— 'generation_result.jsonl'ë¡œ ì €ì¥
    if args.output_file:
        output_path = args.output_file
    else:
        output_path = os.path.join(checkpoint_dir, "generation_result.jsonl")

    print(f"ğŸ’¾ Saving results to {output_path}...")
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
            
    base_name = os.path.splitext(output_path)[0]
    config_save_path = f"{base_name}_config.json"
    
    print(f"âš™ï¸ Saving generation config to {config_save_path}...")
    
    # args ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
    generation_config = vars(args)
    
    # (ì„ íƒ ì‚¬í•­) ë³´ê¸° ì¢‹ê²Œ ì €ì¥ëœ ì ˆëŒ€ ê²½ë¡œë“¤ë„ ì¶”ê°€í•´ì£¼ë©´ ì¢‹ìŠµë‹ˆë‹¤
    generation_config['saved_checkpoint_dir_abs'] = checkpoint_dir
    
    with open(config_save_path, 'w', encoding='utf-8') as f:
        json.dump(generation_config, f, indent=4, ensure_ascii=False)
    # â–²â–²â–² ì„¤ì • ì €ì¥ ì™„ë£Œ â–²â–²â–²

    print("âœ… Generation complete!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate responses using vLLM with trained config")
    
    # í•„ìˆ˜: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì—¬ê¸°ì— config.yamlê³¼ adapter í´ë”ê°€ ìˆì–´ì•¼ í•¨)
    parser.add_argument("--checkpoint_dir", type=str, required=True, help="Path to the checkpoint directory (e.g., outputs/exp/step-1000)")
    
    # ì„ íƒ: ë°ì´í„°ì…‹ (ì§€ì • ì•ˆ í•˜ë©´ configì˜ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì“¸ ìˆ˜ë„ ìˆìŒ)
    parser.add_argument("--dataset_name", type=str, default="anthropic/hh-rlhf", help="Dataset to generate responses for")
    parser.add_argument("--split", type=str, default="test", help="Dataset split")
    parser.add_argument("--prompt_column", type=str, default="prompt", help="Column name for prompts")
    
    # ì„ íƒ: ìƒì„± íŒŒë¼ë¯¸í„°
    parser.add_argument("--n_samples", type=int, default=1, help="Number of samples to generate per prompt")
    parser.add_argument("--max_len", type=int, default=None, help="Max context length (default: use config or 2048)")
    parser.add_argument("--max_new_tokens", type=int, default=512, help="Max new tokens to generate")
    parser.add_argument("--temperature", type=float, default=0.7, help="Sampling temperature")
    parser.add_argument("--top_p", type=float, default=0.9, help="Nucleus sampling top-p")
    
    # ì„ íƒ: ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: checkpoint_dir/generation_result.jsonl)
    parser.add_argument("--output_file", type=str, default=None, help="Custom output file path")

    args = parser.parse_args()
    main(args)